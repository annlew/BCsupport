<!DOCTYPE html>

<html lang="en" data-content_root="../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Checklist &#8212; BCsupport 0.1 documentation</title>
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=5ecbeea2" />
    <link rel="stylesheet" type="text/css" href="../_static/basic.css?v=b08954a9" />
    <link rel="stylesheet" type="text/css" href="../_static/alabaster.css?v=27fed22d" />
    <script src="../_static/documentation_options.js?v=2709fde1"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Swestore" href="../swestore/swestore.html" />
    <link rel="prev" title="Running jobs on Tetralith" href="batchjobs.html" />
   
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  

  
  

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="checklist">
<span id="id1"></span><h1>Checklist<a class="headerlink" href="#checklist" title="Link to this heading">¶</a></h1>
<p>If you develop your own codes/scripts or would like to increase the allocated resources for an existing code, please work through this checklist before submitting your production runs or a large number of jobs.</p>
<ol class="arabic">
<li><p><strong>Is your code/script parallelised?</strong> No -&gt; Use one core: <strong>n 1 not N 1</strong></p></li>
<li><p>If yes -&gt; How is it parallelised?</p>
<p><strong>Can your code make use of more than one node?</strong> Check by first doing short test jobs.</p>
<ul class="simple">
<li><p>Does the job finish quicker when more resources are allocated?</p></li>
<li><p>Does the job make use of all the allocated resources? Use e.g. seff:</p></li>
</ul>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>$seff job_id_of_test_job

Example output
Job ID: 26294158
Cluster: tetralith
User/Group: x_alewi/x_alewi
State: COMPLETED (exit code 0)
Nodes: 4
Cores per node: 32
CPU Utilized: 474-08:22:07
<span class="hll">CPU Efficiency: 99.60% of 476-05:58:24 core-walltime
</span>Job Wall-clock time: 3-17:17:48
Memory Utilized: 166.25 GB (estimated maximum)
Memory Efficiency: 0.00% of 0.00 MB (0.00 MB/node)
</pre></div>
</div>
<p>What is the difference between the core-walltime and Job Wall-clock time here? The Wall time is how long time it takes for your job to finish, in “real time”. The core-walltime (or total number of core hours used for the job) is the wall time multiplied with the number of allocated cores, i.e. 34*4 times 3 days, 17 hours, 17 minutes and 48 seconds in the example above.</p>
<p>Is the code parallelised with MPI, OpenMP or simply running independent processes in parallel? If you are not sure how to allocate the right resources, ask for help.</p>
</li>
<li><p><strong>How does your job scale?</strong> The performance will decrease with increasing number of resources (communication overhead) and there is always a performance-walltime tradeoff. How important is it to have your job finish more quickly? Is the decreased job time worth the extra core hour consumption?</p>
<p>Example scalability test. Blue line shows the walltime for the job depending on the number of cores used. The red line shows ideal scaling. The difference between the blue and red lines represents communication overhead/blocking etc. I would use 32 cores for this job based on the performance test.</p>
<a class="reference internal image-reference" href="../_images/scale.png"><img alt="Missing image file" src="../_images/scale.png" style="width: 600px;" />
</a>
<p>The maximum wall time on Tetralith is 7 days. If your job cannot finish within 7 days, there are workarounds, but these should in general <strong>be avoided</strong> [*]. If your jobs need more than seven days of wall time, please ask for help to set up a plan for how to manage your jobs.</p>
</li>
<li><p>Is your resource usage reasonable? The answer is usually “It depends”. A rule of thumb is:</p>
<p><strong>“Avoid using resources in a way that blocks other users from using them or prevents them from working efficiently”.</strong>
What this means depends on the current workload in the project. Please always check the queue status and current usage before submitting a job</p>
<p>Check the queue of the project:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ squeue -A naiss2024-1-3
</pre></div>
</div>
<p>Check the recent usage</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>$ projinfo
Principal Investigator (PI):   Qiong Zhang
Slurm account:                   naiss2024-1-3
<span class="hll">Current core time allocation:  2000000 h/month
</span>Consumed compute resource time during the last 30 days:

<span class="hll">Total:                                   2001774.68
</span></pre></div>
</div>
<p>Keep checking the queue and core time consumption as your jobs run.</p>
</li>
</ol>
<p>[*] Why should workarounds for extending the wall time beyond seven days be avoided?
On NSC systems it’s possible to use something called boost-tools to tweak job priorities, wall-time limits and create node reservations. However, these tweaks always come at cost. The project pays with boost-tool tokens that are shared amongst all users within the project. In addition to that, by using boost-tools, you make your colleagues within the same project queue longer than what they would if you hadn’t used the boost-tools.</p>
<p>More specifically, the issue associated with extending the wall time beyond seven days, is that you run a risk of not being able to finish your job, and thus waste the used core hour of a job with a wall time of seven days (and “punish” other project members by wasting core-hours). NSC has a policy to inform about planned downtime of their systems with at least seven days notice. That means that a planned downtime might appear after a job that relies on an extended wall time has been started and the job will not finish. Moreover, since the tokens are available to all project members, you can’t rely on tokens being available when you need to extend the wall time. Someone else might have used up the tokens.</p>
<p>If there is no other solution, wall-time extensions can be used, but should be thought through carefully, and not used routinely.</p>
<p>More information about boost-tools can be found here:
<a class="reference external" href="https://www.nsc.liu.se/support/batch-jobs/boost-tools/">https://www.nsc.liu.se/support/batch-jobs/boost-tools/</a></p>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="Main">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../index.html">BCsupport</a></h1>









<search id="searchbox" style="display: none" role="search">
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" placeholder="Search"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script><h3>Navigation</h3>
<p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../modules.html">Modules on Tetralith</a></li>
<li class="toctree-l1"><a class="reference internal" href="../conda.html">Conda on Tetralith</a></li>
<li class="toctree-l1"><a class="reference internal" href="../usage_stats.html">Check usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notebooks/notebooks.html">Notebooks on Tetralith</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="batchjobs.html">Running jobs on Tetralith</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">Checklist</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../swestore/swestore.html">Swestore</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compile_link/compile_link.html">Compile and link</a></li>
<li class="toctree-l1"><a class="reference internal" href="../CMIPworkshop/CMIPworkshop.html">CMIP workshop</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../index.html">Documentation overview</a><ul>
  <li><a href="batchjobs.html">Running jobs on Tetralith</a><ul>
      <li>Previous: <a href="batchjobs.html" title="previous chapter">Running jobs on Tetralith</a></li>
      <li>Next: <a href="../swestore/swestore.html" title="next chapter">Swestore</a></li>
  </ul></li>
  </ul></li>
</ul>
</div>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &#169;2023, Anna.
      
      |
      Powered by <a href="https://www.sphinx-doc.org/">Sphinx 8.2.3</a>
      &amp; <a href="https://alabaster.readthedocs.io">Alabaster 1.0.0</a>
      
      |
      <a href="../_sources/batchjobs/checklist.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>